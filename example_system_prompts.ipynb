{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenClio: Analyzing AI Agent System Prompts\n",
    "\n",
    "This notebook demonstrates how to use OpenClio with Vertex AI to analyze a collection of system prompts for AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install OpenClio from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ggilligan12/openclio.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openclio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Your GCP project ID\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your actual project ID\n",
    "MODEL_NAME = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\" for better quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your System Prompts\n",
    "\n",
    "Replace this with your actual system prompts. Each prompt should be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system prompts (replace with your actual data)\n",
    "system_prompts = [\n",
    "    \"You are a helpful customer support agent for a SaaS company. Help users troubleshoot technical issues with patience and clarity. Always be professional and empathetic.\",\n",
    "    \"You are a creative writing assistant. Help users brainstorm story ideas, develop characters, and refine their prose. Be encouraging and provide constructive feedback.\",\n",
    "    \"You are a code review bot. Analyze code for security vulnerabilities, performance issues, and style violations. Provide specific, actionable feedback with examples.\",\n",
    "    \"You are a financial advisor chatbot. Provide general financial education and guidance. Never give specific investment advice. Always include appropriate disclaimers.\",\n",
    "    \"You are a language learning tutor. Help users practice conversation, explain grammar concepts, and provide vocabulary assistance. Be encouraging and patient.\",\n",
    "    # Add your system prompts here...\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(system_prompts)} system prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI LLM\n",
    "llm = openclio.VertexLLMInterface(\n",
    "    model_name=MODEL_NAME,\n",
    "    project_id=PROJECT_ID,\n",
    "    location=\"us-central1\",\n",
    "    max_output_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "print(\"✓ Models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Clio Analysis\n",
    "\n",
    "This will:\n",
    "1. Extract facets from each system prompt (Primary Purpose, Domain, Key Capabilities, Interaction Style)\n",
    "2. Cluster similar prompts\n",
    "3. Build a hierarchy\n",
    "4. Generate UMAP visualization\n",
    "\n",
    "**Note**: This may take several minutes depending on the number of prompts and Vertex AI API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = openclio.runClio(\n",
    "    facets=openclio.systemPromptFacets,\n",
    "    llm=llm,\n",
    "    embeddingModel=embedding_model,\n",
    "    data=system_prompts,\n",
    "    outputDirectory=\"./clio_output\",\n",
    "    displayWidget=True,\n",
    "    llmBatchSize=10,  # Lower batch size for Vertex AI rate limits\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Results\n",
    "\n",
    "The widget above shows:\n",
    "- **Left**: UMAP plot of all system prompts\n",
    "- **Right Top**: Hierarchical tree of clusters\n",
    "- **Right Bottom**: Text viewer for selected cluster\n",
    "\n",
    "Click on clusters in the tree to view the system prompts in that cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access\n",
    "\n",
    "You can also access the results programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get facet values for a specific prompt\n",
    "prompt_idx = 0\n",
    "print(f\"System Prompt: {system_prompts[prompt_idx][:100]}...\\n\")\n",
    "print(\"Extracted Facets:\")\n",
    "for fv in results.facetValues[prompt_idx].facetValues:\n",
    "    print(f\"  {fv.facet.name}: {fv.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the hierarchy for a specific facet\n",
    "facet_idx = 0  # Primary Purpose facet\n",
    "facet = results.facets[facet_idx]\n",
    "print(f\"Hierarchy for facet: {facet.name}\\n\")\n",
    "\n",
    "if results.rootClusters[facet_idx]:\n",
    "    for i, root_cluster in enumerate(results.rootClusters[facet_idx]):\n",
    "        print(f\"Top-level cluster {i+1}: {root_cluster.name}\")\n",
    "        if root_cluster.children:\n",
    "            for child in root_cluster.children[:3]:  # Show first 3 children\n",
    "                print(f\"  └─ {child.name}\")\n",
    "else:\n",
    "    print(\"No clusters for this facet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "- **Rate Limits**: Vertex AI has rate limits. Use `llmBatchSize=10` or lower if you hit quota errors\n",
    "- **Model Selection**: `gemini-1.5-flash` is faster and cheaper, `gemini-1.5-pro` is more accurate\n",
    "- **Data Size**: For large datasets (>1000 prompts), consider running overnight\n",
    "- **Checkpointing**: Results are cached in `outputDirectory`, so you can resume if interrupted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
